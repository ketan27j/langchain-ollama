# Getting started - local gemma model with ollama using langchain
-Setup ollama locally using https://github.com/ollama/ollama
-Download gemma model locally using 'ollama run llama2'
-Clone this repository locally
-Create environment using 'virtualenv .env'
-Activate environment using '.env/bin/activate'
-Install requirements using 'pip install -r requirements.txt'

## gemma-response.py

Generate response for any query using gemma model and ollama

## gemma-web-response.py

Generate response for any query using gemma model and web based search
